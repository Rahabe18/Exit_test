# -*- coding: utf-8 -*-
"""Final_Coding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z39ic9cTnAjP5rRPvf3-_DihGqsnpKyT

# Emotion Detection in Twitter Data using NLP Techniques
---
---
You have been given with a dataset of Twitter posts (tweets) that are labelled with specific emotions.
- Your objective is to develop a model that can accurately classify these tweets into different emotion categories such as :
  - joy,
  - sadness,
  - anger,
  - fear,
  - surprise etc.

To achieve this, follow the steps outlined below:
- Data Preprocessing
- FeatureExtraction
- ModelBuilding and Evaluation (Create different models, including deep learning models)
- Evaluate the Model's Performance

---
    
- Data Preprocessing

  ● Cleaning the dataset, handling missing values, and preparing the data for analysis.

- Feature Extraction

  ● Use techniques used for transforming the raw text data into numerical features that can be used by machine learning models.

- Model Building and Evaluation

  ● Detail the construction and training of various models, including traditional machine learning models and deep learning models.

● Include the code and explanation for each model.

- Evaluate the Model's Performance

- Present the results and compare the performance of different models.

#1. Data Preprocessing
---
"""

# Import libraries
#------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
#---------------------
df = pd.read_csv('/content/tweet_emotions.csv')

# Display the first few rows
#--------------------
print(df.head())

# Check for missing values
#-------------------------------------------
print(df.isnull().sum())

"""###No missing values found"""

#Statistical analysis of the dataset
#-------------------------------------
df.describe()

#Complete information of the dataset
#-------------------------------------
df.info()

"""###There are 2 categorical columns ['sentiment','content'] and 1 numerical column in the dataset ['tweet_id']"""

#Shape of the dataset
print(df.shape)
#Dimension of the dataset
print(df.ndim)
print(df.size)

#Drop duplicates
#-----------------
df.drop_duplicates(subset=['content'], inplace=True)

#Remove rows where text or label is missing
#-----------------------------------------------
df.dropna(subset=['content','tweet_id','sentiment'],inplace=True)

#shape of datset after removing duplicates
#-------------------------------------------
print(df.shape)

#Simple text pre-processing
#--------------------------------
import re #import regular expression library

def cleaned_text(text):
  text = re.sub(r'[^a-zA-Z\s]','',text) # remove special characters and numbers
  text = text.lower() # convert to lower case
  text = re.sub(r'\s+','',text).strip()#remove whitespaces
  return text

df['cleaned_content'] = df['content'].apply(cleaned_text)
print("\n Cleaned text")
print("*"*(len('cleaned_content')))
print(df[['content','cleaned_content','sentiment']].head())

#Rows after preprocessing
#------------------------------
print("Rows after preprocessing:", len(df))
print(df.head())

"""#2. Feature Extraction
---
Transforming the raw text data into numerical features that can be used by machine learning models-using TF-IDF
"""

#Import libraries
#----------------------
from sklearn.feature_extraction.text import TfidfVectorizer
X_text = df['cleaned_content']
y = df['sentiment']

#Using TF-IDF
#--------------
tfidf_vector = TfidfVectorizer(max_features=5000,ngram_range=(1,2),stop_words='english')
X_tfidf = tfidf_vector.fit_transform(X_text)

#Fit and transform
#-------------------
X_tfidf = tfidf_vector.fit_transform(X_text)
print("After vectorization:",X_tfidf.shape)

"""#3. Model Building and Evaluation
---
"""

#Splitting the data into 70% traing data/15% validation data and 15% testing data
#-----------------------
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score,f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
X_train,X_temp,y_train,y_temp = train_test_split(X_tfidf,y,test_size=0.3,stratify=y,random_state=42)
X_valid,X_test,y_valid,y_test = train_test_split(X_temp,y_temp,test_size=0.5,stratify= y_temp,random_state=42)

print(f"Train data shape: {X_train.shape}")
print(f"Validation data shape: {X_valid.shape}")
print(f"Test data shape: {X_test.shape}")

#Fit on training data
#Performance Evaluation on validation data using Accuracy, Macro-F1, and a classification report
#----------------------------------------------------------------------------
def evaluate_model(model,Xtr,ytr,Xva,yva,title=''):
  #Fit the model on (Xtr,ytr) and evaluate on(Xva,yva)
  model.fit(Xtr,ytr)
  y_predicted = model.predict(Xva)

  accuracy = accuracy_score(yva,y_predicted)
  f1_macro = f1_score(yva,y_predicted,average='macro')
  print(f"\n---Validation results for {title}---")

  print(f"Accuracy:{accuracy}")
  print(f"Macro-F1:{f1_macro}")
  print(classification_report(yva,y_predicted))
  print(confusion_matrix(yva,y_predicted))

  return accuracy,f1_macro,y_predicted

#Refit on training datamand evaluating on testing data using accuracy,macro,classification report
#----------------------------------------------------------
def final_evaluation(model,Xtr,ytr,Xte,yte,title=''):
  model.fit(Xtr,ytr)
  y_predicted = model.predict(Xte)

  accuracy = accuracy_score(yte,y_predicted)
  f1_macro = f1_score(yte,y_predicted,average='macro')

  print(f"\n=======Final Test Results for {title}=========")
  print(f"Accuracy:{accuracy}")
  print(f"Macro-F1:{f1_macro}")
  print(classification_report(yte,y_predicted))
  print(confusion_matrix(yte,y_predicted))

  fig,ax = plt.subplots()
  ConfusionMatrixDisplay.from_predictions(yte,y_predicted,ax=ax)
  ax.set_title(f"Confusion matrix-{title}")
  plt.tight_layout()
  plt.show()

  return accuracy,f1_macro,y_predicted

"""##3.1 Traditional Model-SVC (Support Vector Classifier)"""

from sklearn.svm import LinearSVC #import libraries

svc_model = LinearSVC(class_weight="balanced")
#Evaluation
svc_val_acc, svc_val_f1, _ = evaluate_model(svc_model, X_train, y_train, X_valid, y_valid, "LinearSVC")

#Final test evaluation
final_evaluation(svc_model, X_train, y_train, X_test, y_test, "LinearSVC")

"""---
###RESULT EVALUATION WITH SVC:
---
1. Validation acuuracy = 21%
2. Macro-F1 = 0.03
3. Confusion matrix shows all "worry"

##3.2 Using Naive_Bayes
"""

from sklearn.naive_bayes import MultinomialNB
#Definig model
#--------------------
nb_model = MultinomialNB()
#Validation evaluation
#-----------------------
nb_val_acc, nb_val_f1, _ = evaluate_model(nb_model,X_train,y_train,X_valid,y_valid,"MultinomialNB")

#Final test evaluation
#------------------------
final_evaluation(nb_model,X_train,y_train,X_test,y_test,"MultinomialNB")
#

"""---
###RESULT FOR NAIVE-BAYES:
---
1. Accuracy =21%
2. Macro-F1 = 0.041
3. Mostly "neutral" predictions

#4. Deep Learning based Model

##4.a.Using BERT with Hugging face transformers
"""

!pip install transformers datasets evaluate accelerate -q

import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset,DatasetDict

#Encoding labels
#-------------------
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_valid_enc = label_encoder.transform(y_valid)
y_test_enc = label_encoder.transform(y_test)
num_classes = len(label_encoder.classes_)

# Convert sparse matrices to dense NumPy arrays for Dataset.from_dict
X_train_dense = X_train.toarray()
X_valid_dense = X_valid.toarray()
X_test_dense = X_test.toarray()


#HuggingFace DatasetDict
training_ds = Dataset.from_dict({"text": X_train_dense, "label": y_train_enc})
validation_ds = Dataset.from_dict({"text": X_valid_dense, "label": y_valid_enc})
testing_ds = Dataset.from_dict({"text": X_test_dense, "label": y_test_enc})
dataset = DatasetDict({"train": training_ds, "validation": validation_ds, "test": testing_ds})

